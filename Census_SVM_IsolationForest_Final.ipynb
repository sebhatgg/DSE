{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ceb976",
   "metadata": {},
   "source": [
    "#### The development of a hybrid anomaly detection model begins by loading and cleaning the dataset to remove inconsistencies and irrelevant data. Proceed with selecting the most predictive features for anomaly detection. Divide the data into training and testing sets, ensuring that both contain a representative mix of anomalies and normal instances. Apply feature scaling to standardize the range of the data features, which is crucial for many modeling techniques. Fit the hybrid model, which typically integrates multiple anomaly detection methods to leverage their individual strengths. Finally, the model's effectiveness will be evaluated using precision, recall, and the F1-score, and performance will be visually assessed with a confusion matrix. This comprehensive approach ensures that the hybrid model is robust, accurate, and reliable in detecting anomalies across different data scenarios, instilling assurance in its reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a268f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac41b610",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 5)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_45696\\3137386942.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m \u001b[0mhybrid_svm_iso\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_45696\\3137386942.py\u001b[0m in \u001b[0;36mhybrid_svm_iso\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# Train Isolation Forest on the filtered normal data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0miso_forest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIsolationForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontamination\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0miso_forest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_normal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m# Prepare test data by appending SVM predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \"\"\"\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;31m# Pre-sort indices to avoid that each individual tree of the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    803\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    806\u001b[0m                 \u001b[1;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m                 \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 5)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "\n",
    "def hybrid_svm_iso():\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv('adultdata.csv')\n",
    "    \n",
    "    data.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    #convert all the values in the columns into numeric\n",
    "    label_encoder = LabelEncoder()\n",
    "    for column in data.select_dtypes(include=['object']).columns:\n",
    "        data[column] = label_encoder.fit_transform(data[column])\n",
    "    \n",
    "    X = data.drop('fnlwgt', axis=1)  # This is the target variable\n",
    "    y_cont = data['fnlwgt']\n",
    "    \n",
    "    y = np.where(y_cont > 20000, 1, 0)  # Define `threshold` based on the dataset\n",
    "    \n",
    "    # Select top k features; k might be adjusted based on the dataset\n",
    "    selector = SelectKBest(f_classif, k=5)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    \n",
    "      # Use SVM to identify normal data points\n",
    "    svm = SVC(kernel='rbf', gamma='auto')\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    svm_preds = svm.predict(X_train_scaled)\n",
    "    \n",
    "    # Append SVM predictions as a new feature to the scaled data\n",
    "    X_train_scaled_with_pred = np.hstack((X_train_scaled, svm_preds.reshape(-1, 1)))\n",
    "    \n",
    "    # Filter only normal data points as identified by SVM\n",
    "    normal_indices = np.where(svm_preds == 0)[0]  # Assuming '0' is normal\n",
    "    X_train_normal = X_train_scaled[normal_indices]\n",
    "    \n",
    "    # Train Isolation Forest on the filtered normal data\n",
    "    iso_forest = IsolationForest(contamination=0.1, n_estimators=100, random_state=42)\n",
    "    iso_forest.fit(X_train_normal)\n",
    "    \n",
    "    # Prepare test data by appending SVM predictions\n",
    "    svm_preds_test = svm.predict(X_test_scaled)\n",
    "    X_test_scaled_with_pred = np.hstack((X_test_scaled, svm_preds_test.reshape(-1, 1)))\n",
    "    \n",
    "    # Predict and evaluate the model on the test set using Isolation Forest\n",
    "    iso_preds = iso_forest.predict(X_test_scaled_with_pred[:, :-1])  # Exclude the appended SVM predictions\n",
    "    # Convert predictions to match the original label (1 for normal, -1 for anomaly)\n",
    "    iso_preds = np.where(iso_preds == 1, 0, 1)  # Assuming '0' is normal, '1' is anomaly\n",
    "    \n",
    "    # Evaluate the final outcome from Isolation Forest\n",
    "    precision = precision_score(y_test, iso_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, iso_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, iso_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f'Precision: {precision:.3f}')\n",
    "    print(f'Recall: {recall:.3f}')\n",
    "    print(f'F1 Score: {f1:.3f}')\n",
    "    \n",
    "    print(classification_report(y_test, iso_preds))\n",
    "    \n",
    "    \n",
    "    # Use a confusion matrix for visualization of the outcome\n",
    "    \n",
    "    cm = confusion_matrix(y_test, iso_preds, labels=[0, 1])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Anomaly', 'Normal'])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    \n",
    " \n",
    "hybrid_svm_iso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48a1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
